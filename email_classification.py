# -*- coding: utf-8 -*-
"""email-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JuTJOcbt3GtaEuxhi5_HGM1uS_v2vEh
"""

# !rm -rf /content/*
!pip install --upgrade --no-cache-dir gdown
#https://drive.google.com/file/d/1tFWUoD4mxga4KTVRRQcF8ZuIZD-KkgMr/view?usp=sharing
#https://drive.google.com/file/d/1RrXMsrppQnKb0Absnr9EDQ4wygAYvOnh/view?usp=sharing
#https://drive.google.com/file/d/1_JEJ-2uszpe9p_eOEHYRE9fkEmElgng5/view?usp=sharing
#https://drive.google.com/file/d/1kNZ7dt8M0sxk5aSLA3utzqG4XS_czyaP/view?usp=sharing
!gdown https://drive.google.com/uc?id=1_JEJ-2uszpe9p_eOEHYRE9fkEmElgng5
!gdown https://drive.google.com/uc?id=1kNZ7dt8M0sxk5aSLA3utzqG4XS_czyaP
!gdown https://drive.google.com/uc?id=1tFWUoD4mxga4KTVRRQcF8ZuIZD-KkgMr
!gdown https://drive.google.com/uc?id=1RrXMsrppQnKb0Absnr9EDQ4wygAYvOnh
!unzip /content/archive.zip
!rm -rf /content/archive.zip

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

"""### Import necessary libraries"""

import multiprocessing
import seaborn as sns
import email
import matplotlib.pyplot as plt

"""### Load Data"""

df = pd.read_csv("/content/emails.csv")

idx = 112233
with open('/content/sample_email.txt','w') as f:
    f.write(df['message'].values[idx])

"""### Data Exploration"""

# view first 5 rows of the dataset
df.head()

# get shape of the data
df.shape

# a sample email
print(df.loc[1]['message'])

# transform the email into correct format
message = df.loc[1]['message']
e = email.message_from_string(message)

e.items()

# get date
e.get('Date')

# show message body
e.get_payload()

"""### Extract headers"""

# now we add those fields into our 'df' dataframe
def get_field(field, messages):
    column = []
    for message in messages:
        e = email.message_from_string(message)
        column.append(e.get(field))
    return column

df['date'] = get_field("Date", df['message'])
df['subject'] = get_field("Subject", df['message'])
df['X-Folder'] = get_field("X-Folder", df['message'])
df['X-From'] = get_field("X-From", df['message'])
df['X-To'] = get_field("X-To", df['message'])
df.head(3)

"""### Extract Message Body"""

def body(messages):
    column = []
    for message in messages:
        e = email.message_from_string(message)
        column.append(e.get_payload())
    return column

df['body'] = body(df['message'])
df.head(3)

"""### Employee Names"""

df['file'][:10]

def employee(file):
    column = []
    for string in file:
        column.append(string.split("/")[0])
    return column

df['employee'] = employee(df['file'])
df.head(3)

"""### Look into X-Folder"""

print("number of folders: ", df.shape[0])
print("number of unique folders: ", df['X-Folder'].unique().shape[0])

unique_emails = pd.DataFrame(df['X-Folder'].value_counts())
unique_emails.reset_index(inplace=True)

unique_emails.columns = ['folder_name', 'count']
# top 20 folders
unique_emails.iloc[:20,:]

"""### Visualize top 20 folders"""

plt.figure(figsize=(10,6))
sns.barplot(x='count', y='folder_name', data=unique_emails.iloc[:20, :], palette="Blues_d")
plt.title("Top 20 folders")
plt.xlabel("Count")
plt.ylabel("Folder_Name")
plt.show()

"""**Let's see top 20 highest email sender employee**"""

top_20 = pd.DataFrame(df['employee'].value_counts()[:20])
top_20.reset_index(inplace=True)
top_20.columns = ["Employee_name", "Counts"]
top_20

"""**Visualize top 20 highest email sender employees**"""

plt.figure(figsize=(10,8))

sns.barplot(y="Employee_name", x="Counts", data=top_20, palette="Blues_d")
plt.title("Top 20 highest email sender employee")
plt.xlabel("Count")
plt.ylabel("Employee_name")
plt.show()

"""## Data Cleaning and Transformation

**date : column**
"""

import datetime
from dateutil import parser

# this is sample example
x = parser.parse("Fri, 4 May 2001 13:51:00 -0700 (PDT)")
print(x.strftime("%d-%m-%Y %H:%M:%S"))

def change_type(dates):
    column = []
    
    for date in dates:
        column.append(parser.parse(date).strftime("%d-%m-%Y %H:%M:%S"))
    return column

df['date'] = change_type(df['date'])
df.head(2)

"""**Column : X-Folder**"""

print(df['X-Folder'][0])

# we only want last folder name
df['X-Folder'][0].split("\\")[-1]

def preprocess_folder(folders):
    column = []
    for folder in folders:
        if (folder is None or folder == ""):
            column.append(np.nan)
        else:
            column.append(folder.split("\\")[-1].lower())
    return column

df['X-Folder'] = preprocess_folder(df['X-Folder'])
df.head(2)

# count unique folders
print("Unique Foldes: ", len(df['X-Folder'].unique()))

# view some of them
df['X-Folder'].unique()[0:20]

"""**Replace empty missing values in subject with np.nan**"""

def replace_empty_with_nan(subject):
    column = []
    for val in subject:
        if (val == ""):
            column.append(np.nan) 
        else:
            column.append(val)
    return column

df['subject'] = replace_empty_with_nan(df['subject'])
df['X-To'] = replace_empty_with_nan(df['X-To'])

df.isnull().sum()

# calculate percentage of missing values
miss = df.isnull().sum()
miss = miss[miss>0]
miss = miss / df.shape[0]
miss

# drop missing value rows
df.dropna(axis=0, inplace=True)

df.isnull().sum(), df.shape

df.head(3)

"""Drop the following columns:
- file
- message
- date
- X-From
- X-To
- employee
"""

cols_to_drop = ['file','message','date','X-From','X-To','employee']

df.drop(cols_to_drop, axis=1, inplace=True)

df.head()

# save the data
df.to_csv("/content/cleaned_data.csv", index=False)

"""## Enron Email Classification using Machine Learning

"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

"""### Import necessary libraries"""

import matplotlib.pyplot as plt
import re
import string
import time
pd.set_option('display.max_rows', 50)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')

"""### Load Data"""

df = pd.read_csv("/content/cleaned_data.csv")

# view first 5 rows of the dataframe
df.head()

"""### Data Pre-processing

#### Remove Folders
Remove folders that do not contain enough e-mails because such folders would not be significant for training our classifier. Also, we can infer that some folders with very little e-mails in them were created but unused.
"""

def remove_folders(emails, n):
    # returns the number of folders containing more than 'n' number of emails
    email_count = dict(df['X-Folder'].value_counts())
    small_folders = [key for key, val in email_count.items() if val<=n]
    emails = df.loc[~df['X-Folder'].isin(small_folders)]
    return emails

n = 150
df = remove_folders(df, n)

print("Total folders: ", len(df['X-Folder'].unique()))
print("df.shape: ", df.shape)

"""**Combine subject and body columns**"""

df['text'] = df['subject'] + " " + df['body']

# drop the columns 'subject' and 'body'
df.drop(['subject','body'], axis=1, inplace=True)

"""Now, do the following to preprocess text:
- lowercasing all words
- Remove extra new lines
- Remove extra tabs, punctuations, commas
- Remove extra white spaces
- Remove stopwords
"""

def preprocess(x):
    # lowercasing all the words
    x = x.lower()
    
    # remove extra new lines
    x = re.sub(r'\n+', ' ', x)
    
    # removing (replacing with empty spaces actually) all the punctuations
    x = re.sub("["+string.punctuation+"]", " ", x)
    
    # remove extra white spaces
    x = re.sub(r'\s+', ' ', x)
    
    return x

start = time.time()
df.loc[:,'text'] = df.loc[:, 'text'].map(preprocess)

# remove stopwords
df.loc[:, 'text'] = df.loc[:, 'text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))
end = time.time()
print("Execution time (sec): ",(end - start))

"""**Data Filtering**"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

data = pd.read_csv("/content/preprocessed.csv")

# view first 5 rows of the dataframe
data.head()

data['X-Folder'].value_counts().head(50)

labels = ['management','calender','logistics','corporate','online trading','universities','it']
data = data[data['X-Folder'].isin(labels)].reset_index(drop=True)
data.head()

data.shape

!pip install sentence-transformers

data.columns

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
embeddings = model.encode(sentences)
print(embeddings.shape)

from tqdm.auto import tqdm
text_data = data['text'].values.tolist()

data_emb = []
for i in tqdm(text_data):
    data_emb.append(model.encode(i).ravel().tolist())

df = pd.DataFrame(data_emb)
df['class'] = data['X-Folder'].values.tolist()
df.head()

df.to_csv('emb_data.csv',index=False)

"""**Data Modelling**"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

df = pd.read_csv('/content/emb_data.csv')

X = df.drop(columns = ['class'])
y = df[['class']]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101, stratify=y)

X_train.shape,X_test.shape

y_train.shape,y_test.shape

!pip install scikit-plot

#importing ml libraries
import time
import scikitplot as skplt
import matplotlib.pyplot as plt
import numpy as np
from sklearn.base import clone
from sklearn.metrics import classification_report,balanced_accuracy_score,confusion_matrix
#defining class varaiable
label = y_test['class'].unique().tolist()
model_obj = {}

#Taining and testing logistic regression model

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(class_weight='balanced',n_jobs=-1)
print('Results for LogisticRegression:-\n')
start_time = time.time()
#training
lr.fit(X_train,y_train)
svc_time = (time.time() - start_time)
print('*'*80) 
#displaying training time
print('\nTraining time(sec) = ',svc_time)

#prediction on train data
y_pred1 = lr.predict(X_train)

start_time = time.time()
#prediction on test data
y_pred = lr.predict(X_test)
svc_time2 = (time.time() - start_time)
#displaying prediction time
print('Prediction time(sec) = ',svc_time2)

#defining confusion matrix and accuracy score
cm_svc = confusion_matrix(y_test, y_pred)
svc_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_svc = balanced_accuracy_score(y_train,y_pred1)
acc2_svc = balanced_accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 
model_obj[acc2_svc] = clone(lr)
#displaying accurcay score
print('\nTraining score = ',acc1_svc)
print('Tesing score = ',acc2_svc)
print('\n')
print('*'*80)  

#displaying misclassification rate
print('\n')
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_svc[i])-cm_svc[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',svc_miss)
print('Total % of missclassified points (test data) = ',(svc_miss/len(y_test))*100)
print('\n')
print('*'*80) 

#confusion matrix plot
print('\n\nConfusion matrix:')
skplt.metrics.plot_confusion_matrix(y_test, y_pred,x_tick_rotation=90)
plt.show()
print('\n')
print('*'*80) 

#Classification report
print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

#storing the metrics
LR = [acc1_svc, acc2_svc, svc_miss, svc_miss/len(y_test), svc_time, svc_time2]

#Taining and testing descision tree model

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(criterion='gini',          #or 'entropy'
                             splitter='best',           #or 'random'
                             max_depth=None,            #or 10,15,20
                             max_features=None,         #or 0.5,0.6,0.7,0.8
                             class_weight='balanced') 

print('Results for Decision Tree classifier:-\n')
start_time = time.time()
#training
dtc.fit(X_train,y_train)
dtc_time = (time.time() - start_time)
print('*'*80) 
#displaying training time
print('\nTraining time(sec) = ',dtc_time)

#prediction on train data
y_pred1 = dtc.predict(X_train)

start_time = time.time()
#prediction on test data
y_pred = dtc.predict(X_test)
dtc_time2 = (time.time() - start_time)
#displaying prediction time
print('Prediction time(sec) = ',dtc_time2)

#defining confusion matrix and accuracy score
cm_dtc = confusion_matrix(y_test, y_pred)
dtc_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_dtc = balanced_accuracy_score(y_train,y_pred1)
acc2_dtc = balanced_accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 
model_obj[acc2_dtc] = clone(dtc)
#displaying accurcay score
print('\nTraining score = ',acc1_dtc)
print('Tesing score = ',acc2_dtc)
print('\n')
print('*'*80)  

#displaying misclassification rate
print('\n')
label = np.unique(y_train.values)
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_dtc[i])-cm_dtc[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',dtc_miss)
print('Total % of missclassified points (test data) = ',(dtc_miss/len(y_test))*100)
print('\n')
print('*'*80) 

#display confusion matrix
print('\n\nConfusion matrix:')
skplt.metrics.plot_confusion_matrix(y_test, y_pred,x_tick_rotation=90)
plt.show()
print('\n')
print('*'*80) 

#display classification report
print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

#saving all metrics
DTC = [acc1_dtc, acc2_dtc, dtc_miss, dtc_miss/len(y_test),dtc_time, dtc_time2]

#Taining and testing support vector classifier model
from sklearn.svm import LinearSVC

svc = LinearSVC(C=1.0,class_weight='balanced')

print('Results for support vector classifier:-\n')
start_time = time.time()
#training
svc.fit(X_train,y_train)
svc_time = (time.time() - start_time)
print('*'*80) 
#displaying training time
print('\nTraining time(sec) = ',svc_time)
#prediction on train data
y_pred1 = svc.predict(X_train)

start_time = time.time()
#prediction on test data
y_pred = svc.predict(X_test)
svc_time2 = (time.time() - start_time)
#displaying prediction time
print('Prediction time(sec) = ',svc_time2)

#defining confusion matrix and accuracy score
cm_svc = confusion_matrix(y_test, y_pred)
svc_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_svc = balanced_accuracy_score(y_train,y_pred1)
acc2_svc = balanced_accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 
model_obj[acc2_svc] = clone(svc)
#displaying accurcay score
print('\nTraining score = ',acc1_svc)
print('Tesing score = ',acc2_svc)
print('\n')
print('*'*80)  

#displaying misclassification rate
print('\n')
label = np.unique(y_train.values.ravel())
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_svc[i])-cm_svc[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',svc_miss)
print('Total % of missclassified points (test data) = ',(svc_miss/len(y_test))*100)
print('\n')
print('*'*80) 

#display confusion matrix
print('\n\nConfusion matrix:')
skplt.metrics.plot_confusion_matrix(y_test, y_pred,x_tick_rotation=90)
plt.show()
print('\n')
print('*'*80) 

#display classification report
print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

#saving all metrics
SVC = [acc1_svc, acc2_svc, svc_miss, svc_miss/len(y_test), svc_time, svc_time2]

#Taining and testing ExtraTreesClassifier model
from sklearn.ensemble import ExtraTreesClassifier

svc = ExtraTreesClassifier(n_estimators=100,class_weight='balanced')

print('Results for Extra-Trees Classifier:-\n')
start_time = time.time()
#training
svc.fit(X_train,y_train)
svc_time = (time.time() - start_time)
print('*'*80) 
#displaying training time
print('\nTraining time(sec) = ',svc_time)

#prediction on train data
y_pred1 = svc.predict(X_train)

start_time = time.time()
#prediction on test data
y_pred = svc.predict(X_test)
svc_time2 = (time.time() - start_time)
#displaying prediction time
print('Prediction time(sec) = ',svc_time2)

#defining confusion matrix and accuracy score
cm_svc = confusion_matrix(y_test, y_pred)
svc_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_svc = balanced_accuracy_score(y_train,y_pred1)
acc2_svc = balanced_accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 
model_obj[acc2_svc] = clone(svc)
#displaying accurcay score
print('\nTraining score = ',acc1_svc)
print('Tesing score = ',acc2_svc)
print('\n')
print('*'*80)  

#displaying misclassification rate
print('\n')
label = np.unique(y_train.values.ravel())
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_svc[i])-cm_svc[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',svc_miss)
print('Total % of missclassified points (test data) = ',(svc_miss/len(y_test))*100)
print('\n')
print('*'*80) 

#display confusion matrix
print('\n\nConfusion matrix:')
skplt.metrics.plot_confusion_matrix(y_test, y_pred,x_tick_rotation=90)
plt.show()
print('\n')
print('*'*80) 

#display classification report
print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

#saving all metrics
ETC = [acc1_svc, acc2_svc, svc_miss, svc_miss/len(y_test), svc_time, svc_time2]

#Taining and testing RandomForestClassifier model
from sklearn.ensemble import RandomForestClassifier

svc = RandomForestClassifier(n_estimators=100,class_weight='balanced')

print('Results for RandomForest Classifier:-\n')
start_time = time.time()
#training
svc.fit(X_train,y_train)
svc_time = (time.time() - start_time)
print('*'*80) 
#displaying training time
print('\nTraining time(sec) = ',svc_time)

#prediction on train data
y_pred1 = svc.predict(X_train)

start_time = time.time()
#prediction on test data
y_pred = svc.predict(X_test)
svc_time2 = (time.time() - start_time)
#displaying prediction time
print('Prediction time(sec) = ',svc_time2)

#defining confusion matrix and accuracy score
cm_svc = confusion_matrix(y_test, y_pred)
svc_miss = np.sum(y_pred!=y_test.values.ravel())
acc1_svc = balanced_accuracy_score(y_train,y_pred1)
acc2_svc = balanced_accuracy_score(y_test,y_pred)
print('\n')
print('*'*80) 
model_obj[acc2_svc] = clone(svc)
#displaying accurcay score
print('\nTraining score = ',acc1_svc)
print('Tesing score = ',acc2_svc)
print('\n')
print('*'*80)  

#displaying misclassification rate
print('\n')
label = np.unique(y_train.values.ravel())
for i in range(len(np.unique(y_train.values.ravel()))):
    err = np.sum(cm_svc[i])-cm_svc[i][i]
    print('No of missclassified for class {} (test data) = {} '.format(label[i],err))
print('-'*65)   
print('Total no of missclassified points (test data) = ',svc_miss)
print('Total % of missclassified points (test data) = ',(svc_miss/len(y_test))*100)
print('\n')
print('*'*80) 

#display confusion matrix
print('\n\nConfusion matrix:')
skplt.metrics.plot_confusion_matrix(y_test, y_pred,x_tick_rotation=90)
plt.show()
print('\n')
print('*'*80) 

#display classification report
print('\n\nClassification report:-\n')
print(classification_report(y_test,y_pred))
print('\n')
print('*'*80) 

#saving all metrics
RFC = [acc1_svc, acc2_svc, svc_miss, svc_miss/len(y_test), svc_time, svc_time2]

#Displaying all saved results in onr final dataframe

s1 = [LR,DTC,SVC,ETC,RFC]
s2 = ['LR','DTC','SVC','ETC','RFC']
col=['Train score','Test score','No of Missclassification','% of Missclasification','Training time','Prediction time']
result=dict.fromkeys(s2,None)

for i in range(len(s1)):
    result[s2[i]] = s1[i]
    
temp = pd.DataFrame.from_dict(result, orient='index',columns=col).reset_index()
result = temp.rename(columns={'index':'Classifiers'})
result['% of Missclasification'] *= 100
result

x = np.arange(len(s1))
plt.figure(figsize=(10,5))
plt.bar(x+0.2, result['Train score'], color ='green',width = 0.4)
plt.bar(x-0.2, result['Test score'], color ='red',width = 0.4)
plt.xticks(x, result.Classifiers.values.tolist())
plt.xticks(rotation=90)
plt.legend(['Train score','Test score'])
plt.title('Models vs score', fontsize = 20)
plt.xlabel('Models', fontsize = 15)
plt.ylabel('Score', fontsize = 15)
plt.grid()
plt.show()

#Plotting test score of all algos
plt.bar(x, result['Test score'], color ='green',width = 0.4)
plt.xticks(x, result.Classifiers.values.tolist())
plt.legend(['Test score'])
plt.xticks(rotation=90)
plt.grid()
plt.show()

import warnings
warnings.filterwarnings("ignore") 

best_model = None
max_score = 0
for i,j in model_obj.items():
    if i >= max_score:
        best_model = clone(j)
        max_score = i
        best_model.fit(X_train,y_train.values.ravel())

best_model

import joblib
filename = 'model.sav'
joblib.dump(best_model, filename)

"""**Web app**"""

!pip install sentence-transformers 
!pip install streamlit==1.20.0

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import streamlit as st
# from sentence_transformers import SentenceTransformer
# import email
# import uuid
# import numpy as np
# import joblib
# import re
# import string
# import warnings  
# warnings.filterwarnings("ignore")   
# 
# @st.cache_resource  
# def load_models():
#     emb_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
#     pred_model = joblib.load('model.sav')
#     return emb_model,pred_model
# 
# def preprocess(x):
#     # lowercasing all the words
#     x = x.lower()
#     
#     # remove extra new lines
#     x = re.sub(r'\n+', ' ', x)
#     
#     # removing (replacing with empty spaces actually) all the punctuations
#     x = re.sub("["+string.punctuation+"]", " ", x)
#     
#     # remove extra white spaces
#     x = re.sub(r'\s+', ' ', x)
#     
#     return x
# 
# emb_model,pred_model = load_models()
# 
# def predict_email_class(path):
#     with open(path, 'r') as f:
#         s = f.read()
# 
#     e = email.message_from_string(s)
#     body = e.get_payload()
#     subject = e.get('Subject')
#     s = subject + ' ' + body
#     s = preprocess(s)
#     arr = emb_model.encode(s).ravel()
#     cls = pred_model.predict([arr])[0]
#     return cls
# 
# 
# def predict_email_class2(subject,body):
#     s = subject + ' ' + body
#     s = preprocess(s)
#     arr = emb_model.encode(s).ravel()
#     cls = pred_model.predict([arr])[0]
#     return cls
# 
# st.title("Email Classification Web App")
# c = st.selectbox(label='select', options=['upload','type'], index=0)
# file = None
# if c == 'upload':
#     # Create file uploader
#     file = st.file_uploader("Upload a email file", type=["txt"])
# else:
#     sender_email = st.text_input(label='Enter sender of email')
#     receiver_email = st.text_input(label='Enter receiver of email')
#     subject = st.text_input(label='Enter subject of email')
#     body = st.text_input(label='Enter body of email')
#     cc1 = st.text_input(label='Enter cc1 of email')
#     cc2 = st.text_input(label='Enter cc2 of email')
#     cc3 = st.text_input(label='Enter cc3 of email')
# 
# # Save file locally
# if st.button('Predict'):
#     if file is not None:
#         file_contents = file.read().decode("utf-8")
#         with open("/content/email.txt", "w") as f:
#             f.write(file_contents)
#         s = file_contents
#         pred = predict_email_class('/content/email.txt')
#     else:
#         l = [sender_email,receiver_email,subject,cc1,cc2,cc3,body]
#         with open('/content/email.txt','w') as f:
#             for i in l:
#                 if i:
#                     f.write(i+'\n')
#         label_names = ['sender_email','receiver_email','cc1','cc2','cc3','subject','body']
#         s = ''
#         for i,j in zip(l,label_names):
#             if i:
#                 s+=j+' : '+i+'\n'
#         pred = predict_email_class2(subject,body)
#     with st.expander("See Input"):
#         for i in s.split('\n'):
#             st.write(i)
#     st.success(f'Prediction from model : {pred}')
#     if not os.path.isdir(f'/content/{pred}'):
#         os.mkdir(f'/content/{pred}')
#     os.rename('/content/email.txt',f'/content/{pred}/email-{uuid.uuid4().hex}.txt')

!streamlit run app.py --server.port 8113 & npx localtunnel --port 8113

!python -V

